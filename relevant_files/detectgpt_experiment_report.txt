DetectGPT Experiment Report

Introduction
------------
This report outlines the methodology and findings of an experiment designed to test the DetectGPT model's ability to discern whether text was generated by a 'victim' model or a 'spoof' model. The experiment aimed to simulate a scenario where an attacker might attempt to spoof text to appear as if it originated from a watermarked Large Language Model (LLM).

Methodology
-----------
The experiment involved several key steps:
1. A 'victim' model, TinyLlama-1.1B-Chat-v1.0, was used to generate a dataset of text samples with the KGW watermark applied.
2. A 'spoof' model, GPT-2 1.5B, was finetuned on the outputs of the victim model, focusing only on the final layer to adhere to compute limitations.
3. The finetuned spoof model was then used to generate adversarially modified spoofs, resulting in two datasets: harmful and non-harmful spoofed texts.
4. The DetectGPT analysis was performed by creating perturbations of the generated spoofed texts and comparing the log probabilities of the original and perturbed texts using the victim model.

Results
-------
The analysis yielded the following results after rerunning the experiments with the corrected KGW watermarking process:
- True Positive Rate (TPR) for harmful texts: 0.0
- False Negative Rate (FNR) for harmful texts: 1.0
- True Negative Rate (TNR) for non-harmful texts: 1.0
- False Positive Rate (FPR) for non-harmful texts: 0.0

The updated results indicate that the DetectGPT model's ability to identify harmful texts as coming from the spoof model has remained the same (TPR of 0.0). The identification of non-harmful texts as coming from the victim model has remained the same (TNR of 1.0). The FNR of 1.0 for harmful texts suggests that the same proportion of harmful texts were incorrectly identified as coming from the victim model, indicating no change in the model's ability to detect spoofed texts.

Discussion
----------
The findings suggest that while DetectGPT shows promise in detecting spoofed texts, the methodology could be refined for more robust analysis. The perturbation method, in particular, could be improved by implementing more sophisticated techniques such as paraphrasing or masking out parts of the text.

Further research could explore the impact of different perturbation methods on the model's accuracy and the development of more advanced techniques to strengthen the model's resilience against spoofing attacks.

Conclusion
----------
The experiment demonstrated the potential of DetectGPT in identifying spoofed texts. However, it also highlighted the need for further development to enhance its effectiveness in more complex scenarios. The ability to accurately attribute text to the correct source model is crucial in maintaining the integrity and trustworthiness of LLMs, especially as they become increasingly prevalent in various applications.
